# CNN 图像分割中一些常见的知识点整理
## 残差结构
残差结构的概念来自Deep Residual Learning for Image Recognition这篇文章。首先来介绍一下这篇文章。
网络深度的重要性：
因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富。并且，越深的网络提取的特征越抽象，越具有语义信息。
我们认为是随着网络层数的加深，网络越来越复杂，而复杂的网络更有利于特征的抽取，因此能够达到更好的效果。而实际情况并不是这样，对于原来的网络，如果简单地增加深度，会导致梯度弥散或梯度爆炸。

> 对于该问题的解决方法是使用正则化初始化和中间正则化层（Batch Normalizetion）,这样可以训练几十层的网络。

而现在却出现另一个问题，就是退化问题。不同于过拟合，过拟合的情况是在训练集下的效果很好，而在测试集上的效果不好，但是目前的情况是即使在测试集上的准确率也会随着层数的加深降低。

![ResNet block](./2.png)

如图所示，有两个不同深度的网络一个是56层，另一个是20层，我们可以看出，随着层数的增多，不论是训练集还是测试及，误差error都在增大，这就是退化现象。

吴恩达的课程长一张图也可以说明
![ResNet block](./4.png)
### Residual block的主要结构（残差块）
![ResNet block](./1.png)

上图是一个Residual block结构，我们可以看到有一个x跳过了一层卷积层，这个就是一个shortcut connection。假设residual mappting（残差映射）为$H(x)$，而我们让非线性的映射表示为$F(x)=H(x)-x$，$x$则为identity mappting（恒等映射），则$H(x)=F(x)+x$。我们认为，残差映射要比传统的映射更好优化，因为当极端条件下，当非线性映射为0时，即$F(x) =0$，实质上就是恒等映射。在通常情况下，要使一个网络的一些层去拟合一个恒等映射是很复杂的，这可能就是退化的原因，但是我们可以转化为学一个残差是$F(x)=H(x)-x=0$，实质上就是一个恒等映射$H(x)=x$。拟合残差可能比拟合恒等更加容易。

> 一些其他的解释
> F是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是$F'(5)=5.1$，引入残差后是$H(5)=5.1$, $H(5)=F(5)+5$, $F(5)=0.1$。这里的$F'$和$F$都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如$s$输出从5.1变到5.2，映射$F'$的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射$F$是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化，看到残差网络我第一反应就是差分放大器
> 至于为何shortcut的输入时X，而不是X/2或是其他形式，文章的作者又做了其他的实验如下图所示
> ![image](./3.png)
> 实验的结果发现使用X的效果更好
> 有一种更好理解的方法说明残差网络更容易训练，其实还是这个shortcut，也可以称作skip connection。当他收到某一层传来的梯度后，因为是从之前很早的层直接连接过来的，因此梯度可以直接传过去。

这种残差结构可以通过前向神经网络+shortcut连接实现。而其shortcut连接相当于简单执行了同等映射，不会产生额外的参数，也不会增加计算的复杂度。而且，这个网络可以依旧通过端到端的反响传播进行训练。

ResNet提出了两种残差单元

![ResNet block](./5.png)
其中作图被用在ResNet-34而右图被用在ResNet-50/101/152网络中。在左图网络中通道书比较少，在右图网络中通道数比较大。如果通道数比较大，那么用左图的残差结构就会产生几何级倍数的参数数量，因此可以先用1\*1的卷积将通道数降下来，完成卷积操作后再用1\*1的卷积恢复到原来的通道数，这样的结构成为bottleneck layer。
现在假设一个通道数为256的网络，通过一个残差结构，通过一个残差结构，如果不是用bottlenect那么参数的数量为$3*3*256*256*2=1179648$，而使用了bottleneck的参数数量为$1*1*256*64+3*3*64*64+1*1*64*256=69632$

如果$F(x)$与x的通道数不同怎么办？
+ 如果通道数相同，则$y=F(x)\oplus x$
+ 通道数不同，则$y=F(x)\oplus Wx$，其中$W$为卷积操作，用来调整通道数（1*1的卷积）

shortcut connection的优点：
+ 减轻了梯度弥散的问题，有利于训练更快收敛，因为梯度可以直接先前传递
+ 加强了特征的重用和传递
+ 减少参数

数学证明：
https://blog.csdn.net/u013709270/article/details/78838875

## Inception Module
Imception Mudule是GoogLeNet提出的结构，出自论文[Going deeper with convolutions](https://arxiv.org/pdf/1409.4842.pdf)，基本思想是不需要人为决定使用哪种过滤器或是否需要池化，而由网络自行确定。我们需要做的是给网络传入这些待选的参数（如$1*1，3*3，5*5$， maxpooling），然后把输出连接起来，让网络自己学需要什么样的参数或哪些参数的组合。
## Barch Normailzation
Batch Normailzation这个概念是Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift这篇文章提出的，目前这个算法已经被大量应用图像分割领域，许多方法的网络结构中都是用了它。

尽管随机梯度下降法对于训练深度网络简单高效，但是它有个毛病，就是需要我们人为的去选择参数，比如学习率、参数初始化、权重衰减系数、Drop out比例等。这些参数的选择对训练结果至关重要，以至于我们很多时间都浪费在这些的调参上。

！[image](./6.png)

Batch Normalization的强大指出有以下几点：
+ 你可以选择比较大的初始学习率，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；
+ 你再也不用去理会过拟合中dropout、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；
+ 



1. xception
2. deconvolution
3. 空洞卷积
4. skip connection
5. drop out
6. full convolution
7.  bottleneck layer
8.  uppooling
9.  辅助分支
10. feature map
11. CT值阶段
12. dice Coefficient
13. 假阳性
14. 平衡像素值
15. pooling mask
16. 连通区域分析
17. identity transformation
