# CNN 图像分割中一些常见的知识点整理
## 残差结构
残差结构的概念来自Deep Residual Learning for Image Recognition这篇文章。首先来介绍一下这篇文章。
网络深度的重要性：
因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富。并且，越深的网络提取的特征越抽象，越具有语义信息。
我们认为是随着网络层数的加深，网络越来越复杂，而复杂的网络更有利于特征的抽取，因此能够达到更好的效果。而实际情况并不是这样，对于原来的网络，如果简单地增加深度，会导致梯度弥散或梯度爆炸。

> 对于该问题的解决方法是使用正则化初始化和中间正则化层（Batch Normalizetion）,这样可以训练几十层的网络。

而现在却出现另一个问题，就是退化问题。不同于过拟合，过拟合的情况是在训练集下的效果很好，而在测试集上的效果不好，但是目前的情况是即使在测试集上的准确率也会随着层数的加深降低。

![ResNet block](./2.png)

如图所示，有两个不同深度的网络一个是56层，另一个是20层，我们可以看出，随着层数的增多，不论是训练集还是测试及，误差error都在增大，这就是退化现象。

吴恩达的课程长一张图也可以说明

![ResNet block](./4.png)
### Residual block的主要结构（残差块）
![ResNet block](./1.png)

上图是一个Residual block结构，我们可以看到有一个x跳过了一层卷积层，这个就是一个shortcut connection。假设residual mappting（残差映射）为$H(x)$，而我们让非线性的映射表示为$F(x)=H(x)-x$，$x$则为identity mappting（恒等映射），则$H(x)=F(x)+x$。我们认为，残差映射要比传统的映射更好优化，因为当极端条件下，当非线性映射为0时，即$F(x) =0$，实质上就是恒等映射。在通常情况下，要使一个网络的一些层去拟合一个恒等映射是很复杂的，这可能就是退化的原因，但是我们可以转化为学一个残差是$F(x)=H(x)-x=0$，实质上就是一个恒等映射$H(x)=x$。拟合残差可能比拟合恒等更加容易。

> 一些其他的解释
> F是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是$F'(5)=5.1$，引入残差后是$H(5)=5.1$, $H(5)=F(5)+5$, $F(5)=0.1$。这里的$F'$和$F$都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如$s$输出从5.1变到5.2，映射$F'$的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射$F$是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化，看到残差网络我第一反应就是差分放大器
> 至于为何shortcut的输入时X，而不是X/2或是其他形式，文章的作者又做了其他的实验如下图所示
> ![image](./3.png)
> 实验的结果发现使用X的效果更好
> 有一种更好理解的方法说明残差网络更容易训练，其实还是这个shortcut，也可以称作skip connection。当他收到某一层传来的梯度后，因为是从之前很早的层直接连接过来的，因此梯度可以直接传过去。

这种残差结构可以通过前向神经网络+shortcut连接实现。而其shortcut连接相当于简单执行了同等映射，不会产生额外的参数，也不会增加计算的复杂度。而且，这个网络可以依旧通过端到端的反响传播进行训练。

ResNet提出了两种残差单元

![ResNet block](./5.png)

其中作图被用在ResNet-34而右图被用在ResNet-50/101/152网络中。在左图网络中通道书比较少，在右图网络中通道数比较大。如果通道数比较大，那么用左图的残差结构就会产生几何级倍数的参数数量，因此可以先用1\*1的卷积将通道数降下来，完成卷积操作后再用1\*1的卷积恢复到原来的通道数，这样的结构成为bottleneck layer。
现在假设一个通道数为256的网络，通过一个残差结构，通过一个残差结构，如果不是用bottlenect那么参数的数量为$3*3*256*256*2=1179648$，而使用了bottleneck的参数数量为$1*1*256*64+3*3*64*64+1*1*64*256=69632$

如果$F(x)$与x的通道数不同怎么办？
+ 如果通道数相同，则$y=F(x)\oplus x$
+ 通道数不同，则$y=F(x)\oplus Wx$，其中$W$为卷积操作，用来调整通道数（1*1的卷积）

shortcut connection的优点：
+ 减轻了梯度弥散的问题，有利于训练更快收敛，因为梯度可以直接先前传递
+ 加强了特征的重用和传递
+ 减少参数

数学证明：
https://blog.csdn.net/u013709270/article/details/78838875

## Inception Module
Imception Mudule最早是在[Going deeper with convolutions](https://arxiv.org/abs/1409.4842)一文中提出的结构，基本思想是不需要人为决定使用哪种过滤器或是否需要池化，而由网络自行确定。我们需要做的是给网络传入这些待选的参数（如$1*1，3*3，5*5$， maxpooling），然后把输出连接起来，让网络自己学需要什么样的参数或哪些参数的组合。

inception结构的改进历程

### 最初始的Inception结构
最初的Inception结构是[Going deeper with convolutions](https://arxiv.org/abs/1409.4842)，在这篇文章中，作者提出了目前图像分割主要面临的问题：
1. **由于信息位置的巨大差异，为卷积操作选择合适的卷积核大小就比较困难**。信息分布更全局性的图像偏好较大的卷积核，信息分布比较局部的图像偏好较小的卷积核。
2. 非常深的网络更容易过拟合，将梯度更新传输到整个网络是很困难的。
3. 简单地堆叠较大的卷积层非常消耗计算资源。

为了解决这些存在的问题，提出了inception结构作为解决方案：

下图是最初的Inception结构

![inception](./6.png)

使用了三个不同大小的滤波器（$１×１,３×３,５×５$）对输入执行卷积操作，此外，他还会执行最大池化。所有子层的输出最后会被级联起来，并传送至下一个inception block。
> 采用不同大小的卷积核意味着不同大小的感受野，最后拼接以为只不同尺度特征的融合
> 之所以采用1，3和5，主要是为了方便对齐。设定军妓补偿stride=1之后，只要分别设定padding=0,1,2，那么卷积之后便可以得到相同维度的特征，然后这些特征就可以拼接在一起了
> 由于池化操作对于当前卷积网络的性能至关重要，因此建议在每个incepition 模块中都添加一个并行的池化层。
> 随着inception 模块的叠加，与他们的输出相关的统计数据也会发生变化：随着更高层次捕获更好层次的特征，他们的空间浓度也会下降，这表明在高层的网络应该增加$3*3和5*5$的数量。
> 即使在上面的结构中，使用$5*5$的卷积核也是十分昂贵的，加入了池化层，这种问题更加明显：他们输出的过滤器的数量等与前一阶段过滤器的数量。池化层的输出与卷基层的输出合并将不可避免地增加输出的数量。

由此出现了下面的一种inception模块。

![inception](./7.png)

由于传统的卷积神经网络需要消耗大量的计算资源。为了降低算力成本，作者在$3*3和5*5$卷积层之前添加额外的$1*1$卷积层，来限制输入信道的数量，这样做可以极大地减少参数空间大小，有利于降低算力成本。
> 1*1的卷积在池化层之后，而不是之前

### ！inception计算代价的计算代价：
![inception](./8.png)

使用这种结构，我们需要计算的参数数量为$5*5*192*28*28*32=120422400$

![inception](./9.png)

使用这种结构，我们需要计算的参数数量为$1*1*192*28*28*16+5*5*16*28*28*32=12443648$

我们可以清楚的看到参数数量少了一个数量级

----
### Inception结构的改进
出自论文[Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567v3.pdf)

在VGG用两个$3*3$的卷积代替一个$5*5$的卷积后，构建更深的网络逐渐成为主流，但是模型的变大也使得计算效率越来越低，文章试图找到一种方法在扩大网络的同时有尽可能地发挥计算性能。

改进方法
+ 将 5×5 的卷积分解为两个 3×3 的卷积运算以提升计算速度。尽管这有点违反直觉，但一个 5×5 的卷积在计算成本上是一个 3×3 卷积的 2.78 倍。所以叠加两个 3×3 卷积实际上在性能上会有所提升，如图所示：
    + 将全连接的$5*5$的卷积分解，发现它很像两个全连接的$3*3$的卷积
    + ![inception](./12.png)    
    + ![inception](./10.png)
    > 在两个 3×3 的卷积和后使用非线性激活还可一提高性能

### 再次改进
+ 任何尺寸大于$3*3$的卷积和够可以改写成多个$3*3$的卷积，我们是否可以将$3*3$变得更小？答案是肯定的，但是文章作者发现$2*2$的对称卷积核效果不如给对称卷积核，如n×1。事实上，我们可以认为，可以用1×n的卷积核替换任何n×n的卷积和。将$n*n$ 的卷积核尺寸分解为 1×n 和 n×1 两个卷积。一个 3×3 的卷积等价于首先执行一个 1×3 的卷积再执行一个 3×1 的卷积。这种方法在成本上要比单个 3×3 的卷积降低 33%，如图所示：
    + ![inception](./11.png)
    > 任意的 $n*n$ 的卷积都可以通过1xn卷积后接nx1卷积来替代。实际上，作者发现在网络的前期使用这种分解效果并不好，还有在中度大小的feature map上使用效果才会更好。feature map（$m*m$）的大小，m在12到20之间。
    > 另外还有一种将$1*n$与$n*1$并联的设计，如下图所示
    >
    > ![inception](./13.png)
+ 注意representational bottlenecks问题
  
由于深度CNN网络会不断使用pooling来减少feature maps的尺寸，从而达到增大感受野提取特征的目的。但这样做必然会使信息丢失，一般为了减少信息的过度丢失，在加入pooling层减少feature map的尺寸的同时同比例扩大channels的数目。
> 实行的方法有以下两种
> + 先将将channels的数目扩大（通常使用$1*1$的卷积），然后在使用pooling层来减少feature map的尺寸。不过因为有了$1*1$的卷积，这回导致计算量变得非常大
> + 先做pooling减小feature map的尺寸，然后再用$1*1$的卷积对其channels数目放大。不过先使用pooling还是会丢失信息。
> 两种方法如图所示：
> ![image](./14.png)
> 
作者提出了一种新的方法用来解决此问题：

使用两个并行的结构，$C$和$P$。其中$P$是池化层（平均池化或最大池化），$C$是卷积层。结构如下图所示：

![image](./15.png)
> 其中左图为该结构中各部分的细节，有图为该结构中各个部分的得到的特征图尺寸。这种作为可以极大地减少参数的数量，因为有了卷积层的加入，不会丢失掉信息。

### 最新的inception结构
在[Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/pdf/1602.07261.pdf)一文中，作者收到了同一时期效果较好的ResNet网络的结构启发，讲残差结构融入了原先的inception结构，构建了inception-v4和inception-ResNet网络。

其中inception-v4网络，使用三个inception模块，如图所示：

![image](./16.png)

![image](./17.png)

![image](./18.png)

其中这三张图都是这钱的方法提到的，在inception v4的网络里，先使用第一张图的模型在底层，在网络的中间使用第二张图的结构，在网络快要结束时，使用第三张图的机构。

此外，在inception v4的网络中还是用了reduction module结构，用来调整feature map的尺寸和深度，而inception结构不改变尺寸和大小，这是与之前inception结构的区别

在inception-RerNet的网络中，残差结构被真正应用。inception-ResNet网络有两个版本，v1与v2。这两个版本的网络的整体结构是相同的，只是每个部分之中有细微的差异。
在inception-ResNet v1网络中，inception结构有三个，如下图所示：

![image](./19.png)

![image](./20.png)

![image](./21.png)

我们可以看到，图中所有的inception结构去掉了pooling层，而变成了本层的输入直接shortcut到输出，并与其他并行卷积的结构做了连接，没有池化首先能保证图像尺寸不变，并且不会丢失细节，而且由于使用了残差连接的思想，使得图像的细节能够进一步保留到下一层。图中1*1的卷积为了改变通道数

inception-ResNet v2网络中，inception结构同样有三个，如下图所示：

![image](./22.png)

![image](./23.png)

![image](./24.png)

相对于v1，v2仅仅是对通道数做了改变，将之前固定的通道数改成变化的

！ 这里只是讲了inception结构的演变，关于网络的改进之后再讲

# xception
https://www.leiphone.com/news/201708/KGJYBHXPwsRYMhWw.html
xception是extreme inception的意思，在inception的基础上进一步提高了效率，减小了参数

传统卷积中一个卷积核就需要同时学习空间相关性和通道间的相关性，空间相关性学习的是某个特征在空间中的分布，通道间的相关性是这些不同特征的组合方式。

通常，在一组特征图上进行卷积需要三维的卷积核，也即卷积核需要同时学习空间上的相关性和通道间的相关性。将这两种相关性显式地分离开来，是Inception模块的思想之一：Inception模块首先使用$1*1$的卷积核将特征图的各个通道映射到一个新的空间，在这一过程中学习通道间的相关性；再通过常规的$3*3$或$5*5$的卷积核进行卷积，以同时学习空间上的相关性和通道间的相关性。

但此时，通道间的相关性和空间相关性仍旧没有完全分离，也即$3*3$或$5*5$的卷积核仍然是多通道输入的，那么是否可以假设它们可以被完全分离？显然，当所有$3*3$或$5*5$的卷积都作用在只有一个通道的特征图上时，通道间的相关性和空间上的相关性即达到了完全分离的效果。

Xception更进一步，不在只是将输入数据分割成几个压缩的数据快。而是为每个输出通道单独映射空间相关性，然后在实行$1*1$的深度方面的卷积来获取跨通道的相关性。

对于inception的改进，下图为inception v3的机构

![image](./25.png)

假设出一个简化版的inception module

![image](./26.png)

进一步假设，吧第一部分的3个$1*1$的卷积核统一起来，后面的3个$3*3$的卷积核分别负责一部分通道

![image](./27.png)

最后推广到极端形式，

![image](./28.png)

对每一个通道都进行卷积，之后在合并起来

该模块讲输入数据在channel维度上进行解耦合，该模块城市为"extrame" version of Inception module，这个思想是和depth-wise convolution非常相似的。

但Xception与原版的Depth-wise convolution有两个不同之处
1. 原版Depth-wise convolution, 先主通道卷积，在$1*1$卷积；二Xception是反过来，先$1*1$卷积，在逐通道卷积
2. 原版Depth-wise convolution的两个卷积之间不带激活函数，二Xception在经过1*1卷积之后会带上一个Relu的非线性激活函数



## depthwise separable convolution深度可分离卷积
https://blog.csdn.net/weixin_38668159/article/details/80415626

+ depth-wise convolution：每一层分别作卷积
+ point-wise convolution ：1x1卷积，把各个层连接起来

## 辅助分类器(Auxiliary Classifier)
Going deeper with convolutions一文中提出。在创建GoogLeNet是，为了在深度较深的网络中增强梯度像所有层传播的能力。

辅助分类器的产生最初的目的是将有用的梯度直接先底层传送，使这些梯度可以被立即使用，并且通过在非常深的网络中消除梯度消失问题来改善训练期间的收敛性。然而作者在之后的文章提到，辅助分类器在训练早期并未有使收敛性得到改善，带不带辅助分类器的网络的效果差异不大，但是在训练过程快结束时，带有辅助分类器的网路性能开始超过不带辅助分类器的网络。另外，当去掉底层的辅助分类器，发现性能并没有多少改善，因此作者认为之前提到的辅助分类器可以更好地将参数传递给底层的假设是错误的。相反，可以认为辅助分类器可以充当正则化器。

带有辅助分支的网络应该尽量放在网络的中间部位及以后，其损失函数也应该由主干网络分支的损失和所有辅助网络损失加权构成。

![image](./29.png)

如图所示，左侧网络路径为主干网络，右侧为辅助网络，其输出也可成为侧输出（side output），


<!-- 
Inception v3在v2的基础上：
+ RMSProp 优化器；
+ Factorized 7x7 卷积；
+ 辅助分类器使用了 BatchNorm；
+ 标签平滑（添加到损失公式的一种正则化项，旨在阻止网络对某一类别过分自信，即阻止过拟合）。 -->



## Batch Normailzation
Batch Normailzation这个概念是Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift这篇文章提出的，目前这个算法已经被大量应用图像分割领域，许多方法的网络结构中都是用了它。

尽管随机梯度下降法对于训练深度网络简单高效，但是它有个毛病，就是需要我们人为的去选择参数，比如学习率、参数初始化、权重衰减系数、Drop out比例等。这些参数的选择对训练结果至关重要，以至于我们很多时间都浪费在这些的调参上。

由于最初的Inception模块


Batch Normalization的强大指出有以下几点：
+ 你可以选择比较大的初始学习率，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；
+ 你再也不用去理会过拟合中dropout、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；
+ 


## filter 过滤器
就是指卷积核

## deconvolution
deconvolution又称为反卷积（转置卷积），这个概念是在Deconvolutional networks（2010）提出的，后来在deconv net网络得到了在语义分割领域的应用

Deconvolution大致可以分为以下几个方面：
1. unsupervised learning，其实就是covolutional sparse coding[1][2]：这里的deconv只是观念上和传统的conv反向，传统的conv是从图片生成feature map，而deconv是用unsupervised的方法找到一组kernel和feature map，让它们重建图片。
2. CNN可视化[3]：通过deconv将CNN中conv得到的feature map还原到像素空间，以观察特定的feature map对哪些pattern的图片敏感，这里的deconv其实不是conv的可逆运算，只是conv的transpose，所以tensorflow里一般取名叫transpose_conv。
3. upsampling[4][5]：在pixel-wise prediction比如image segmentation[4]以及image generation[5]中，由于需要做原始图片尺寸空间的预测，而卷积由于stride往往会降低图片size， 所以往往需要通过upsampling的方法来还原到原始图片尺寸，deconv就充当了一个upsampling的角色。

其中upsampling是图像的语义分割中最常用的。

转置卷积的动态演示：
https://github.com/vdumoulin/conv_arithmetic

通过这个动态演示可以更清晰地理解转置卷积。

转置卷积的解释：
https://www.zhihu.com/question/43609045

反卷积可视化以各层得到的特征图作为输入，进行反卷积，得到反卷积结果，用以验证显示各层提取到的特征图。举个例子：假如你想要查看Alexnet 的conv5提取到了什么东西，我们就用conv5的特征图后面接一个反卷积网络，然后通过：反池化、反激活、反卷积，这样的一个过程，把本来一张$13*13$大小的特征图(conv5大小为$13*13$)，放大回去，最后得到一张与原始输入图片一样大小的图片($227*227$)。
## unpooling
简介：unpooling（反池化）与deconvolotion一起构成了densenet网络，其中，dense net'中在max-pooling的过程中，保留了激活的位置，写入文件pooling mask。在上采样的过程中，先按照mask恢复图像，没有值的位置填0。然后再使用deconvolition



## 空洞卷积
空洞卷积是


 

1. 空洞卷积
2. skip connection
3. dropout
4.  bottleneck layer
5.  uppooling
6.  feature map
7.  CT值阶段
8.  dice Coefficient
9.  假阳性
10. 平衡像素值
11. 连通区域分析
12. 蒸馏法
13. representational bottlenecks
14. 空间金字塔池化
15. 分group操作
16. xavier
17. rbm：受限玻尔兹曼机
