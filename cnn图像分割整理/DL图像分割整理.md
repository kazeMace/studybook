# CNN 图像分割中一些常见的知识点整理
## Residual Connection
Residual Connection的概念来自Deep Residual Learning for Image Recognition这篇文章。首先来介绍一下这篇文章。
网络深度的重要性：
因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富。并且，越深的网络提取的特征越抽象，越具有语义信息。
我们认为是随着网络层数的加深，网络越来越复杂，而复杂的网络更有利于特征的抽取，因此能够达到更好的效果。而实际情况并不是这样，对于原来的网络，如果简单地增加深度，会导致梯度弥散或梯度爆炸。

> 对于该问题的解决方法是使用正则化初始化和中间正则化层（Batch Normalizetion）,这样可以训练几十层的网络。

而现在却出现另一个问题，就是退化问题。不同于过拟合，过拟合的情况是在训练集下的效果很好，而在测试集上的效果不好，但是目前的情况是即使在测试集上的准确率也会随着层数的加深降低。


<!-- 这个网络的优点有以下几点：
1. -->

![ResNet block](./2.png)

如图所示，有两个不同深度的网络一个是56层，另一个是20层，我们可以看出，随着层数的增多，不论是训练集还是测试及，误差error都在增大，这就是退化现象。

吴恩达的课程长一张图也可以说明
![ResNet block](./4.png)
### Residual block的主要结构（残差块）
![ResNet block](./1.png)

上图是一个Residual block结构，我们可以看到有一个x跳过了一层卷积层，这个就是一个shortcut connection。假设residual mappting（残差映射）为$H(x)$，而我们让非线性的映射表示为$F(x)=H(x)-x$，$x$则为identity mappting（恒等映射），则$H(x)=F(x)+x$。我们认为，残差映射要比传统的映射更好优化，因为当极端条件下，当非线性映射为0时，即$F(x) =0$，实质上就是恒等映射。在通常情况下，要使一个网络的一些层去拟合一个恒等映射是很复杂的，这可能就是退化的原因，但是我们可以转化为学一个残差是$F(x)=H(x)-x=0$，实质上就是一个恒等映射$H(x)=x$。拟合残差可能比拟合恒等更加容易。

> 一些其他的解释
> F是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是$F'(5)=5.1$，引入残差后是$H(5)=5.1$, $H(5)=F(5)+5$, $F(5)=0.1$。这里的$F'$和$F$都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如$s$输出从5.1变到5.2，映射$F'$的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射$F$是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化，看到残差网络我第一反应就是差分放大器
> 至于为何shortcut的输入时X，而不是X/2或是其他形式，文章的作者又做了其他的实验如下图所示
> ![image](./3.png)
> 实验的结果发现使用X的效果更好
> 有一种更好理解的方法说明残差网络更容易训练，其实还是这个shortcut，也可以称作skip connection。当他收到某一层传来的梯度后，因为是从之前很早的层直接连接过来的，因此梯度可以直接传过去。

这种残差结构可以通过前向神经网络+shortcut连接实现。而其shortcut连接相当于简单执行了同等映射，不会产生额外的参数，也不会增加计算的复杂度。而且，这个网络可以依旧通过端到端的反响传播进行训练。

ResNet提出了两种残差单元

![ResNet block](./5.png)
其中作图被用在ResNet-34而右图被用在ResNet-50/101/152网络中。在左图网络中通道书比较少，在右图网络中通道数比较大。如果通道数比较大，那么用左图的残差结构就会产生几何级倍数的参数数量，因此可以先用1\*1的卷积将通道数降下来，完成卷积操作后再用1\*1的卷积恢复到原来的通道数，这样的结构成为bottleneck layer。
现在假设一个通道数为256的网络，通过一个残差结构，通过一个残差结构，如果不是用bottlenect那么参数的数量为$3*3*256*256*2=1179648$，而使用了bottleneck的参数数量为$1*1*256*64+3*3*64*64+1*1*64*256=69632$

如果$F(x)$与x的通道数不同怎么办？
+ 如果通道数相同，则$y=F(x)\oplus x$
+ 通道数不同，则$y=F(x)\oplus Wx$，其中$W$为卷积操作，用来调整通道数（1*1的卷积）

shortcut connection的优点：
+ 减轻了梯度弥散的问题，有利于训练更快收敛，因为梯度可以直接先前传递
+ 加强了特征的重用和传递
+ 减少参数

数学证明：
https://blog.csdn.net/u013709270/article/details/78838875

## inception

1. inception
2. xception
3. deconvolution
4. 空洞卷积
5. skip connection
6. drop out
7. full convolution
8.  bottleneck layer
9.  uppooling
10. 辅助分支
11. feature map
12. CT值阶段
13. dice Coefficient
14. 假阳性
15. 平衡像素值
16. pooling mask
17. 连通区域分析
18. identity transformation
